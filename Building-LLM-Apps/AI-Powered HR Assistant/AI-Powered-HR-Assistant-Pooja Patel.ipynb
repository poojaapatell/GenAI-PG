{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_openai_api_key(api_key: str):\n",
    "    \"\"\"\n",
    "    Load OpenAI API key from environment variables and set it for the OpenAI API.\n",
    "    Raise an error if the API key is not found.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")  # Fetch the OpenAI API key from environment\n",
    "    if api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key  # Set it as an environment variable for use by OpenAI\n",
    "    else:\n",
    "        # If the API key is not found, raise a descriptive error\n",
    "        raise ValueError(\"OPENAI_API_KEY is not set in the environment variables. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_pdf(pdf_path: str, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Load the PDF file and split it into smaller chunks for processing.\n",
    "    The chunks will be used for embedding and vector search.\n",
    "    \n",
    "    Parameters:\n",
    "    - pdf_path: path to the PDF document.\n",
    "    - chunk_size: size of each chunk.\n",
    "    - chunk_overlap: how much overlap there should be between chunks to preserve context.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of text chunks split from the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the PDF using PyPDFLoader\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        if not documents:\n",
    "            # If the PDF is empty, raise an error\n",
    "            raise ValueError(\"No content found in the PDF file.\")\n",
    "        \n",
    "        # Use RecursiveCharacterTextSplitter to split the documents into smaller chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.split_documents(documents)  # Split the document into chunks\n",
    "        return chunks\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the PDF file is not found\n",
    "        raise FileNotFoundError(f\"PDF file not found at: {pdf_path}\")\n",
    "    except Exception as e:\n",
    "        # Catch any other errors and provide a meaningful message\n",
    "        raise RuntimeError(f\"An error occurred while loading or splitting the PDF: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(chunks, persist_directory=None):\n",
    "    \"\"\"\n",
    "    Create a vector store from the document chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks: List of document chunks to index.\n",
    "    - persist_directory: Path where the FAISS index should be saved. If None, it will be in memory.\n",
    "\n",
    "    Returns:\n",
    "    - A FAISS vector store for performing similarity-based searches.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = OpenAIEmbeddings()  # Create OpenAI embeddings to convert text into vector form\n",
    "\n",
    "        # Create the FAISS vector store from the document chunks and embeddings\n",
    "        vectorstore = FAISS.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding\n",
    "        )\n",
    "        \n",
    "        if persist_directory:\n",
    "            # Save the FAISS index to a directory if persist_directory is provided\n",
    "            vectorstore.save_local(persist_directory)\n",
    "\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        # Catch any errors related to FAISS and embeddings\n",
    "        raise RuntimeError(f\"An error occurred while creating the vector store: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qa_chain(vectorstore):\n",
    "    \"\"\"\n",
    "    Build a QA chain that combines a language model (LLM) and a retriever (vector store).\n",
    "    This chain will retrieve the most relevant documents from the vector store \n",
    "    and use the LLM to generate answers to user queries.\n",
    "\n",
    "    Parameters:\n",
    "    - vectorstore: The FAISS vector store used to retrieve relevant context.\n",
    "\n",
    "    Returns:\n",
    "    - A RetrievalQA chain for answering queries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the prompt template that will be used for question answering\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"\n",
    "You are a helpful assistant answering questions about Nestlé's HR Policy.\n",
    "\n",
    "Use only the information from the provided document context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        # Set up the RetrievalQA chain that uses the vector store and OpenAI model\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0),  # LLM for generating responses\n",
    "            retriever=vectorstore.as_retriever(),  # Retrieve relevant context from the FAISS vector store\n",
    "            chain_type=\"stuff\",  # The type of retrieval chain used\n",
    "            chain_type_kwargs={\"prompt\": prompt_template},  # Provide the prompt to the LLM\n",
    "            return_source_documents=False  # We do not need to return the source documents\n",
    "        )\n",
    "        \n",
    "        return qa_chain\n",
    "    except Exception as e:\n",
    "        # Catch any errors related to the creation of the QA chain\n",
    "        raise RuntimeError(f\"An error occurred while building the QA chain: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str, qa_chain):\n",
    "    \"\"\"\n",
    "    Answer a query using the provided QA chain.\n",
    "\n",
    "    Parameters:\n",
    "    - query: The user's question to be answered.\n",
    "    - qa_chain: The QA chain that will process the query and return an answer.\n",
    "\n",
    "    Returns:\n",
    "    - The generated answer to the query.\n",
    "    \"\"\"\n",
    "    if not query or len(query.strip()) < 5:\n",
    "        # If the query is too short or empty, prompt the user for more information\n",
    "        return \"Please enter a more specific question.\"\n",
    "    \n",
    "    try:\n",
    "        # Run the query through the QA chain and return the result\n",
    "        answer = qa_chain.run(query)\n",
    "        return answer\n",
    "    except ValueError as e:\n",
    "        # Handle any value-related errors\n",
    "        return f\"Error with the provided input: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        # Catch all other errors and display the error message\n",
    "        return f\"An error occurred while processing your query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_chatbot_interface(qa_chain):\n",
    "    \"\"\"\n",
    "    Launch a Gradio interface for the chatbot.\n",
    "    Users can interact with the chatbot through this interface.\n",
    "\n",
    "    Parameters:\n",
    "    - qa_chain: The QA chain that will handle the user's queries.\n",
    "    \"\"\"\n",
    "    gr.Interface(\n",
    "        fn=lambda query: answer_query(query, qa_chain),  # Function to handle user input and provide answers\n",
    "        inputs=gr.Textbox(label=\"Ask a question about Nestlé's HR Policy\"),  # User input for questions\n",
    "        outputs=gr.Textbox(label=\"Answer\"),  # The answer displayed to the user\n",
    "        title=\"Nestlé HR Policy Chatbot\",  # Title of the interface\n",
    "        description=\"Ask any question related to the HR policy. The chatbot will find and return the best answer from the document.\"\n",
    "    ).launch(share=True)  # Launch the interface with a public link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot_workflow(api_key: str, pdf_path: str):\n",
    "    # Step 1: Set OpenAI key\n",
    "    set_openai_api_key(api_key)\n",
    "\n",
    "    # Step 2: Load & split PDF\n",
    "    chunks = load_and_split_pdf(pdf_path)\n",
    "\n",
    "    # Step 3: Create vector store\n",
    "    vectorstore = create_vectorstore(chunks)\n",
    "\n",
    "    # Step 4: Build QA chain\n",
    "    qa_chain = build_qa_chain(vectorstore)\n",
    "\n",
    "    # Step 5: Launch Gradio\n",
    "    launch_chatbot_interface(qa_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pdf_path = \"Nestle HR Policy.pdf\"\n",
    "run_chatbot_workflow(api_key, pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Homebrew)",
   "language": "python",
   "name": "homebrew-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
